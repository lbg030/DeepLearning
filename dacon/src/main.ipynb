{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.auto import tqdm\n",
    "import timm\n",
    "from data.dataset import train_loader, val_loader, test_loader\n",
    "from config import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"resnet50\", pretrained = True, num_classes = 19).to(DEVICE)\n",
    "le = preprocessing.LabelEncoder()\n",
    "optimizer = optim.Adam(model.parameters(), lr = LR)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max = 32, eta_min = 1e-5)\n",
    "label_map = {str(i): int(i) for i in range(19)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    preds, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in (val_loader):\n",
    "            imgs = imgs.float().to(device)\n",
    "            labels = [[label_map[label] for label in tpl] for tpl in labels]\n",
    "            labels = torch.tensor(labels[0], dtype=torch.long).to(DEVICE)\n",
    "\n",
    "            pred = model(imgs)\n",
    "            loss = criterion(pred, labels)\n",
    "            \n",
    "            preds += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "            true_labels += labels.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "        _val_score = f1_score(true_labels, preds, average='weighted')\n",
    "    \n",
    "    return _val_loss, _val_score\n",
    "\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, scheduler):\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in (range(EPOCH)):\n",
    "        batch = 0\n",
    "        train_loss = []\n",
    "        with tqdm(train_loader, unit='batch') as tepoch:\n",
    "            for imgs, labels in tepoch:\n",
    "                batch += 1\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                imgs = imgs.float().to(DEVICE)\n",
    "                labels = [[label_map[label] for label in tpl] for tpl in labels]\n",
    "                labels = torch.tensor(labels[0], dtype=torch.long).to(DEVICE)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "\n",
    "                    output = model(imgs).to(DEVICE)\n",
    "                    loss = criterion(output, labels)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                tepoch.set_postfix(\n",
    "                    phase=\"Training\",\n",
    "                    loss=loss.item()\n",
    "                )\n",
    "\n",
    "            _val_loss, _val_score = validation(model, criterion, val_loader, DEVICE)\n",
    "            _train_loss = np.mean(train_loss)\n",
    "            print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val Weighted F1 Score : [{_val_score:.5f}]')\n",
    "\n",
    "            scheduler.step()\n",
    "                    \n",
    "            if best_score < _val_score:\n",
    "                best_score = _val_score\n",
    "                best_model = model\n",
    "                print(\"Best_model saved\")\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in tqdm((test_loader)):\n",
    "            imgs = imgs.float().to(device)\n",
    "            \n",
    "            pred = model(imgs)\n",
    "            \n",
    "            preds += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "    \n",
    "    preds = le.inverse_transform(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = inference(infer_model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['label'] = preds\n",
    "submit.loc[submit['label'] == '0', 'label'] = '가구수정'\n",
    "submit.loc[submit['label'] == '1', 'label'] = '걸레받이수정'\n",
    "submit.loc[submit['label'] == '2', 'label'] = '곰팡이'\n",
    "submit.loc[submit['label'] == '3', 'label'] = '꼬임'\n",
    "submit.loc[submit['label'] == '4', 'label'] = '녹오염'\n",
    "submit.loc[submit['label'] == '5', 'label'] = '들뜸'\n",
    "submit.loc[submit['label'] == '6', 'label'] = '면불량'\n",
    "submit.loc[submit['label'] == '7', 'label'] = '몰딩수정'\n",
    "submit.loc[submit['label'] == '8', 'label'] = '반점'\n",
    "submit.loc[submit['label'] == '9', 'label'] = '석고수정'\n",
    "submit.loc[submit['label'] == '10', 'label'] = '오염'\n",
    "submit.loc[submit['label'] == '11', 'label'] = '오타공'\n",
    "submit.loc[submit['label'] == '12', 'label'] = '울음'\n",
    "submit.loc[submit['label'] == '13', 'label'] = '이음부불량'\n",
    "submit.loc[submit['label'] == '14', 'label'] = '창틀,문틀수정'\n",
    "submit.loc[submit['label'] == '15', 'label'] = '터짐'\n",
    "submit.loc[submit['label'] == '16', 'label'] = '틈새과다'\n",
    "submit.loc[submit['label'] == '17', 'label'] = '피스'\n",
    "submit.loc[submit['label'] == '18', 'label'] = '훼손'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./baseline_submit.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\lee97\\anaconda3\\envs\\lens\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\lee97\\anaconda3\\envs\\lens\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\lee97\\anaconda3\\envs\\lens\\lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m test_dataset \u001b[39m=\u001b[39m TestDataset(test[\u001b[39m'\u001b[39m\u001b[39mimg_path\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues, \u001b[39mNone\u001b[39;00m, test_transform)\n\u001b[0;32m     39\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m test_labels \u001b[39m=\u001b[39m le\u001b[39m.\u001b[39mtransform(test[\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\lee97\\anaconda3\\envs\\lens\\lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\lee97\\anaconda3\\envs\\lens\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, transforms=None):\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = Path(self.img_path_list[index])\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)['image']\n",
    "        \n",
    "        if self.label_list is not None:\n",
    "            label = self.label_list[index]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "\n",
    "test_transform = A.Compose([\n",
    "                            A.Resize(360,360),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "test = pd.read_csv('C:/Users/lee97/Documents/GitHub/DeepLearning/dacon/src/data/test.csv')\n",
    "test_dataset = TestDataset(test['img_path'].values, None, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_labels = le.transform(test['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = inference(infer_model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mimg_path\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m all_img_list\n\u001b[0;32m      4\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mimg_path\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x : \u001b[39mstr\u001b[39m(x)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m train, val, _, _ \u001b[39m=\u001b[39m train_test_split(df, df[\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m], test_size\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m, stratify\u001b[39m=\u001b[39;49mdf[\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m], random_state\u001b[39m=\u001b[39;49m\u001b[39m41\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m le \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39mLabelEncoder()\n\u001b[0;32m      7\u001b[0m train[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m le\u001b[39m.\u001b[39mfit_transform(train[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\lee97\\anaconda3\\envs\\lens\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2559\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m-> 2562\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[0;32m   2564\u001b[0m )\n\u001b[0;32m   2566\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lee97\\anaconda3\\envs\\lens\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2233\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[0;32m   2235\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2236\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2238\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2239\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2240\u001b[0m     )\n\u001b[0;32m   2242\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "all_img_list = glob.glob('./train/*/*')\n",
    "df = pd.DataFrame(columns=['img_path', 'label'])\n",
    "df['img_path'] = all_img_list\n",
    "df['label'] = df['img_path'].apply(lambda x : str(x).split('\\\\')[1])\n",
    "train, val, _, _ = train_test_split(df, df['label'], test_size=0.3, stratify=df['label'], random_state=41)\n",
    "le = preprocessing.LabelEncoder()\n",
    "train['label'] = le.fit_transform(train['label'])\n",
    "val['label'] = le.transform(val['label'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
