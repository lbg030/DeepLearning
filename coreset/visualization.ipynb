{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def testing(dataloaders):\n",
    "    labeling = {\n",
    "        '0': 0,\n",
    "        '1': 0,\n",
    "        '2': 0,\n",
    "        '3': 0,\n",
    "    }\n",
    "    print(f\"len dataloaders['train'] = {len(dataloaders['train'])}\")\n",
    "    for _ in range(len(dataloaders['train'])):\n",
    "        images, labels = next(iter(dataloaders['train']))\n",
    "        for i in range(4):\n",
    "            images.shape  # torch.Size([32, 1, 28, 28])\n",
    "\n",
    "            labels.shape # torch.Size([32])\n",
    "\n",
    "            import numpy as np\n",
    "            import matplotlib.pyplot as plt\n",
    "            %matplotlib inline\n",
    "\n",
    "            images[i].shape  #  torch.Size([1, 28, 28])\n",
    "\n",
    "            torch_image = torch.squeeze(images[0]) # 0 번째를 없애준다. \n",
    "            torch_image = torch.squeeze(torch_image[0]) \n",
    "            torch_image.shape   #  torch.Size([28, 28])\n",
    "\n",
    "            # 토치를 넘파이화 해줌 \n",
    "            image = torch_image.numpy()\n",
    "            image.shape\n",
    "\n",
    "            label = labels[i].numpy()   # array (9)\n",
    "            labeling[str(int(label))] += 1\n",
    "\n",
    "    print(labeling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE # sklearn 사용하면 easy !! \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def see(model, train_loader):\n",
    "    device = 'cuda:2'\n",
    "    actual = []\n",
    "    deep_features = []\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        images, labels = data.to(device), target.to(device)\n",
    "        features = model['backbone'](images) # 512 차원\n",
    "\n",
    "        deep_features += features.cpu().detach().numpy().tolist()\n",
    "        actual += labels.cpu().numpy().tolist()\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=0) # 사실 easy 함 sklearn 사용하니..\n",
    "    cluster = np.array(tsne.fit_transform(np.array(deep_features)))\n",
    "    actual = np.array(actual)\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    cifar = ['hz','bz','chem','yd']\n",
    "    for i, label in zip(range(10), cifar):\n",
    "        idx = np.where(actual == i)\n",
    "        plt.scatter(cluster[idx, 0], cluster[idx, 1], marker='.', label=label)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GCN Active Learning\n",
    "'''\n",
    "\n",
    "# Python\n",
    "# import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "# import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "# import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import argparse \n",
    "# Custom\n",
    "import models.resnet as resnet\n",
    "from train_test import train, test\n",
    "from load_dataset import load_dataset\n",
    "from selection_methods import query_samples\n",
    "from config import *\n",
    "import timm\n",
    "from pathlib import Path\n",
    "\n",
    "# from data.fst_data import *\n",
    "\n",
    "# import wandb\n",
    "# wandb.init(project=\"classification\", entity=\"lbg030\")\n",
    "# wandb.config = {\n",
    "#   \"learning_rate\": LR,\n",
    "#   \"epochs\": EPOCH,\n",
    "#   \"batch_size\": BATCH\n",
    "# }\n",
    "\n",
    "random.seed(21)\n",
    "\n",
    "# from plotly.subplots import make_subplots\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-l\",\"--lambda_loss\",type=float, default=0.7, \n",
    "#                     help=\"Adjustment graph loss parameter between the labeled and unlabeled\")\n",
    "\n",
    "# parser.add_argument(\"-s\",\"--s_margin\", type=float, default=0.1,\n",
    "#                     help=\"Confidence margin of graph\")\n",
    "\n",
    "# parser.add_argument(\"-n\",\"--hidden_units\", type=int, default=128,\n",
    "#                     help=\"Number of hidden units of the graph\")\n",
    "\n",
    "# parser.add_argument(\"-r\",\"--dropout_rate\", type=float, default=0.3,\n",
    "#                     help=\"Dropout rate of the graph neural network\")\n",
    "parser.add_argument(\"-d\",\"--dataset\", type=str, default=\"fst\",\n",
    "                    help=\"\")\n",
    "\n",
    "parser.add_argument(\"-e\",\"--no_of_epochs\", type=int, default=EPOCH,\n",
    "                    \n",
    "                    help=\"Number of epochs for the active learner\")\n",
    "parser.add_argument(\"-m\",\"--method_type\", type=str, default=\"CoreSet\",\n",
    "                    help=\"\")\n",
    "\n",
    "parser.add_argument(\"-c\",\"--cycles\", type=int, default=CYCLES,\n",
    "                    help=\"Number of active learning cycles\")\n",
    "\n",
    "parser.add_argument(\"-t\",\"--total\", type=bool, default=False,\n",
    "                    help=\"Training on the entire dataset\")\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "\n",
    "##\n",
    "# Main\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    method = args.method_type\n",
    "    methods = ['CoreSet']\n",
    "    datasets = ['fst',]\n",
    "    assert method in methods, 'No method %s! Try options %s'%(method, methods)\n",
    "    assert args.dataset in datasets, 'No dataset %s! Try options %s'%(args.dataset, datasets)\n",
    "    # Model - create new instance for every cycle so that it resets\n",
    "    \n",
    "    \n",
    "            \n",
    "    # results = open('results_'+str(args.method_type)+\"_\"+args.dataset +'_main'+str(args.cycles)+str(args.total)+'.txt','w')\n",
    "    print(\"Dataset: %s\"%args.dataset)\n",
    "    print(\"Method type:%s\"%method)\n",
    "    if args.total:\n",
    "        TRIALS = 1\n",
    "        CYCLES = 1\n",
    "        \n",
    "    else:\n",
    "        CYCLES = args.cycles\n",
    "        \n",
    "    for trial in range(TRIALS):\n",
    "        res_list = []\n",
    "        # Load training and testing dataset\n",
    "        data_train, data_unlabeled, data_test, adden, NO_CLASSES, no_train = load_dataset(args.dataset)\n",
    "        \n",
    "        # Don't predefine budget size. Configure it in the config.py: ADDENDUM = adden\n",
    "        NUM_TRAIN = no_train\n",
    "        indices = list(range(NUM_TRAIN))\n",
    "        # print(trial, indices)\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        with torch.cuda.device(CUDA_VISIBLE_DEVICES):\n",
    "                \n",
    "                    #resnet18    = vgg11().to(device) \n",
    "                resnet18    = resnet.ResNet18().to(device)\n",
    "\n",
    "        models      = {'backbone': resnet18}\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion      = nn.CrossEntropyLoss()\n",
    "        optim_backbone = optim.SGD(models['backbone'].parameters(), lr=LR, \n",
    "                momentum=MOMENTUM, weight_decay=WDECAY)\n",
    "\n",
    "        sched_backbone = torch.optim.lr_scheduler.CosineAnnealingLR(optim_backbone, T_max=200)\n",
    "        \n",
    "        optimizers = {'backbone': optim_backbone}\n",
    "        schedulers = {'backbone': sched_backbone}\n",
    "            \n",
    "        if args.total:\n",
    "            labeled_set = indices\n",
    "        else:\n",
    "            labeled_set = indices[:ADDENDUM]\n",
    "            unlabeled_set = [x for x in indices if x not in labeled_set]\n",
    "        \n",
    "        train_loader = DataLoader(data_train, batch_size=BATCH, num_workers=4, pin_memory=True, sampler=SubsetRandomSampler(labeled_set), drop_last = True)\n",
    "        test_loader = DataLoader(data_test,num_workers=4, batch_size=1)\n",
    "        \n",
    "        dataloaders  = {'train': train_loader, 'test': test_loader}\n",
    "        \n",
    "        print(f\"data_train = {len(data_train)},  data_test = {len(data_test)}\")\n",
    "        print(f\"no_train = {NUM_TRAIN}\")   \n",
    "        \n",
    "        for cycle in range(CYCLES):\n",
    "            \n",
    "            optim_backbone = optim.SGD(models['backbone'].parameters(), lr=LR, \n",
    "                momentum=MOMENTUM, weight_decay=WDECAY)\n",
    "            \n",
    "            # Randomly sample 10000 unlabeled data points\n",
    "            if not args.total:\n",
    "                random.shuffle(unlabeled_set)\n",
    "                subset = unlabeled_set[:SUBSET]\n",
    "            \n",
    "            testing(dataloaders)\n",
    "    \n",
    "            # Training and testing\n",
    "            train_loss = train(models, method, criterion, optimizers, schedulers, dataloaders, args.no_of_epochs, EPOCHL)\n",
    "            # print(f'train loss : {train_loss:.4f}')\n",
    "            # metrics['train_loss'] = round(train_loss, 4)\n",
    "            # train(models, method, criterion, optimizers, schedulers, dataloaders, args.no_of_epochs, EPOCHL)\n",
    "            metrics, results = test(models, EPOCH, method, dataloaders, mode='test')\n",
    "            print(f\"accuracy : {metrics['accuracy']:.4f}\")\n",
    "            print(f\"f1 score : {metrics['f1_score']:.4f}\")\n",
    "            print(f\"precision : {metrics['precision']:.4f}\")\n",
    "            print(f\"recall : {metrics['recall']:.4f}\")\n",
    "            \n",
    "            # acc_l = metrics['accuracy']\n",
    "            # f1_l = metrics['f1_score']\n",
    "            # pre_l = metrics['precision']\n",
    "            # rec_l = metrics['recall']\n",
    "            \n",
    "            # res_list.append([acc_l,f1_l,pre_l,rec_l])\n",
    "    #         wandb.log({'metric/accuracy' : metrics['accuracy'],\n",
    "    #         'metric/f1_score' : metrics['f1_score'],\n",
    "    #         'metric/precision' : metrics['precision'],\n",
    "    #         'metric/recall' : metrics['recall'],\n",
    "    # })\n",
    "            \n",
    "            print('Trial {}/{} || Cycle {}/{} || Label set size {}'.format(trial+1, TRIALS, cycle+1, CYCLES, len(labeled_set)))\n",
    "            \n",
    "            # # print(labeled_set)\n",
    "            # np.array([method, trial+1, TRIALS, cycle+1, CYCLES, len(labeled_set), acc]).tofile(results, sep=\" \")\n",
    "            # results.write(\"\\n\")\n",
    "\n",
    "            if cycle == (CYCLES-1):\n",
    "                # Reached final training cycle\n",
    "                print(\"Finished.\")\n",
    "                break\n",
    "            \n",
    "            if trial == 0 and cycle == 0:\n",
    "                torch.save(models, str(Path(PATH, 'best.pt')))\n",
    "                best_f1 = 0\n",
    "                \n",
    "            if best_f1 < metrics['f1_score']:\n",
    "                best_f1 = metrics['f1_score']\n",
    "                torch.save(models, str(Path(PATH, 'best.pt')))\n",
    "                \n",
    "            # Get the indices of the unlabeled samples to train on next cycle\n",
    "            arg = query_samples(models, method, data_unlabeled, subset, labeled_set, cycle, args)\n",
    "            \n",
    "            # print(f\"arg = {arg}\")\n",
    "            # Update the labeled dataset and the unlabeled dataset, respectively\n",
    "            labeled_set += list(torch.tensor(subset)[arg][-ADDENDUM:].numpy())\n",
    "            listd = list(torch.tensor(subset)[arg][:-ADDENDUM].numpy()) \n",
    "            unlabeled_set = listd + unlabeled_set[SUBSET:]\n",
    "\n",
    "            dataloaders['train'] = DataLoader(data_train, batch_size=BATCH,sampler=SubsetRandomSampler(labeled_set),\n",
    "                                            pin_memory=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "611d41f9c0d119f726c146a3d3bc40ce419f45abac4b6c5efabc98622bc9d855"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
